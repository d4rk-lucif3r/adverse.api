{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"adverse_trials.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNY1Jo8jLn84vX0fX2qCqmy"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xVhgcuPfcwQ7"},"source":["!pip install Scrapy --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QaFO_WGVjV-5"},"source":["!scrapy startproject news"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-etd-1wjaCn"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMqOiMs-sK1e"},"source":["%cd /content/\r\n","!ls news/news"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJeHO7VRtxEN"},"source":["%%writefile /content/news/news/items.py\r\n","# Define here the models for your scraped items\r\n","#\r\n","# See documentation in:\r\n","# https://docs.scrapy.org/en/latest/topics/items.html\r\n","\r\n","import scrapy\r\n","\r\n","\r\n","class NewsItem(scrapy.Item):\r\n","    # define the fields for your item here like:\r\n","    # name = scrapy.Field()\r\n","    headline = scrapy.Field()\r\n","    url = scrapy.Field()\r\n","    # pass\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XsJtRgUsXJ6"},"source":["%%writefile /content/news/news/pipelines.py\r\n","# Define your item pipelines here\r\n","#\r\n","# Don't forget to add your pipeline to the ITEM_PIPELINES setting\r\n","# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\r\n","\r\n","\r\n","# useful for handling different item types with a single interface\r\n","# from itemadapter import ItemAdapter\r\n","\r\n","\r\n","# class NewsPipeline:\r\n","    # def process_item(self, item, spider):\r\n","        # return item\r\n","\r\n","from scrapy import signals\r\n","from scrapy.exporters import CsvItemExporter\r\n","\r\n","class NewsPipeline(object):\r\n","\r\n","  def __init__(self):\r\n","    self.files = {}\r\n","\r\n","  @classmethod\r\n","  def from_crawler(cls, crawler):\r\n","    pipeline = cls()\r\n","    crawler.signals.connect(pipeline.spider_opened, signals.spider_opened)\r\n","    crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)\r\n","    return pipeline\r\n","\r\n","  def spider_opened(self, spider):\r\n","    file = open('%s_items.csv' % spider.name, 'w+b')\r\n","    self.files[spider] = file\r\n","    self.exporter = CsvItemExporter(file)\r\n","    self.exporter.fields_to_export = ['headline', 'url']\r\n","    self.exporter.start_exporting()\r\n","\r\n","  def spider_closed(self, spider):\r\n","    self.exporter.finish_exporting()\r\n","    file = self.files.pop(spider)\r\n","    file.close()\r\n","\r\n","  def process_item(self, item, spider):\r\n","    self.exporter.export_item(item)\r\n","    return item"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gAeFOzdhs8dC"},"source":["%%writefile /content/news/news/settings.py\r\n","# Scrapy settings for news project\r\n","#\r\n","# For simplicity, this file contains only settings considered important or\r\n","# commonly used. You can find more settings consulting the documentation:\r\n","#\r\n","#     https://docs.scrapy.org/en/latest/topics/settings.html\r\n","#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\r\n","#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\r\n","\r\n","BOT_NAME = 'news'\r\n","\r\n","SPIDER_MODULES = ['news.spiders']\r\n","NEWSPIDER_MODULE = 'news.spiders'\r\n","\r\n","\r\n","# Crawl responsibly by identifying yourself (and your website) on the user-agent\r\n","#USER_AGENT = 'news (+http://www.yourdomain.com)'\r\n","\r\n","# Obey robots.txt rules\r\n","ROBOTSTXT_OBEY = True\r\n","\r\n","# Configure maximum concurrent requests performed by Scrapy (default: 16)\r\n","#CONCURRENT_REQUESTS = 32\r\n","\r\n","# Configure a delay for requests for the same website (default: 0)\r\n","# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\r\n","# See also autothrottle settings and docs\r\n","#DOWNLOAD_DELAY = 3\r\n","# The download delay setting will honor only one of:\r\n","#CONCURRENT_REQUESTS_PER_DOMAIN = 16\r\n","#CONCURRENT_REQUESTS_PER_IP = 16\r\n","\r\n","# Disable cookies (enabled by default)\r\n","#COOKIES_ENABLED = False\r\n","\r\n","# Disable Telnet Console (enabled by default)\r\n","#TELNETCONSOLE_ENABLED = False\r\n","\r\n","# Override the default request headers:\r\n","#DEFAULT_REQUEST_HEADERS = {\r\n","#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\r\n","#   'Accept-Language': 'en',\r\n","#}\r\n","\r\n","# Enable or disable spider middlewares\r\n","# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\r\n","#SPIDER_MIDDLEWARES = {\r\n","#    'news.middlewares.NewsSpiderMiddleware': 543,\r\n","#}\r\n","\r\n","# Enable or disable downloader middlewares\r\n","# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\r\n","#DOWNLOADER_MIDDLEWARES = {\r\n","#    'news.middlewares.NewsDownloaderMiddleware': 543,\r\n","#}\r\n","\r\n","# Enable or disable extensions\r\n","# See https://docs.scrapy.org/en/latest/topics/extensions.html\r\n","#EXTENSIONS = {\r\n","#    'scrapy.extensions.telnet.TelnetConsole': None,\r\n","#}\r\n","\r\n","# Configure item pipelines\r\n","# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\r\n","ITEM_PIPELINES = {\r\n","   'news.pipelines.NewsPipeline': 300,\r\n","}\r\n","\r\n","# Enable and configure the AutoThrottle extension (disabled by default)\r\n","# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\r\n","#AUTOTHROTTLE_ENABLED = True\r\n","# The initial download delay\r\n","#AUTOTHROTTLE_START_DELAY = 5\r\n","# The maximum download delay to be set in case of high latencies\r\n","#AUTOTHROTTLE_MAX_DELAY = 60\r\n","# The average number of requests Scrapy should be sending in parallel to\r\n","# each remote server\r\n","#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\r\n","# Enable showing throttling stats for every response received:\r\n","#AUTOTHROTTLE_DEBUG = False\r\n","\r\n","# Enable and configure HTTP caching (disabled by default)\r\n","# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\r\n","#HTTPCACHE_ENABLED = True\r\n","#HTTPCACHE_EXPIRATION_SECS = 0\r\n","#HTTPCACHE_DIR = 'httpcache'\r\n","#HTTPCACHE_IGNORE_HTTP_CODES = []\r\n","#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3hLRbiijbrD"},"source":["%cd /content/\r\n","!ls news/news/spiders"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"necd1ueLjb7S","executionInfo":{"status":"error","timestamp":1614401280558,"user_tz":-330,"elapsed":1598,"user":{"displayName":"Gaurav Chopra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipwFqb9o99ceeIdkuO4dhrh_TlE4yCVopv2_xWBQ=s64","userId":"09663297046543970124"}},"outputId":"aef61c58-a677-41b2-cff1-d1cd6d3d4b00","colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["%%writefile news/news/spiders/news_spider.py\r\n","import os\r\n","import csv\r\n","\r\n","import scrapy\r\n","from bs4 import BeautifulSoup\r\n","from news.items import NewsItem\r\n","\r\n","def get_urls_from_csv():\r\n","    with open('websites.csv', 'rbU') as csv_file:\r\n","        data = csv.reader(csv_file)\r\n","        scrapurls = []\r\n","        for row in data:\r\n","            scrapurls.append(row)\r\n","        return scrapurls\r\n","\r\n","class NewsSpider(scrapy.Spider):\r\n","    name = \"news\"\r\n","    \r\n","    start_urls = [\r\n","                  \"https://economictimes.indiatimes.com/topic/\",\r\n","                  \"https://timesofindia.indiatimes.com/topic/\",\r\n","                  \"https://www.ndtv.com/search?searchtext=\"\r\n","    ]\r\n","    \r\n","    start_urls = [url+keyword for url in start_urls for keyword in ['black money',\r\n","                                                                    'money laundering', \r\n","                                                                    'money launder', \r\n","                                                                    'lauder the money', \r\n","                                                                    'money-mule', \r\n","                                                                    'money mule', \r\n","                                                                    'Hawala', \r\n","                                                                    'drug-trafficking', \r\n","                                                                    'drug trafficking', \r\n","                                                                    'terror', \r\n","                                                                    'terror financing'\r\n","    ]]\r\n","\r\n","    def parse(self, response):\r\n","      # text data of entire webpage\r\n","      soup = BeautifulSoup(response.text, 'html.parser') # parse\r\n","      # extract body from it\r\n","      soup = soup.body # scrape.parse_soup\r\n","\r\n","      all_links = soup.findAll(\"a\", href=True)\r\n","\r\n","      for link in all_links:\r\n","        news = NewsItem()\r\n","        if link.string and (response.url.split('/')[2] in link['href']):\r\n","          news['headline'] = link.string\r\n","          news['url'] = response.urljoin(link['href'])\r\n","\r\n","          yield news\r\n","\r\n","      # all_links = soup.findAll(\"a\", {'target': '_blank'})\r\n","\r\n","      # for link in all_links:\r\n","      #   news = NewsItem()\r\n","      #   news['headline'] = link.find('h2')\r\n","      #   if news['headline']:\r\n","      #     news['headline'] = news['headline'].string\r\n","      #     news['url'] = response.urljoin(link['href'])\r\n","\r\n","      #     yield news"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Writing news/news/spiders/news_spider.py\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b2a9ca404ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'news/news/spiders/news_spider.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'import os\\nimport csv\\n\\nimport scrapy\\nfrom bs4 import BeautifulSoup\\nfrom news.items import NewsItem\\n\\ndef get_urls_from_csv():\\n    with open(\\'websites.csv\\', \\'rbU\\') as csv_file:\\n        data = csv.reader(csv_file)\\n        scrapurls = []\\n        for row in data:\\n            scrapurls.append(row)\\n        return scrapurls\\n\\nclass NewsSpider(scrapy.Spider):\\n    name = \"news\"\\n    \\n    start_urls = [\\n                  \"https://economictimes.indiatimes.com/topic/\",\\n                  \"https://timesofindia.indiatimes.com/topic/\",\\n                  \"https://www.ndtv.com/search?searchtext=\"\\n    ]\\n    \\n    start_urls = [url+keyword for url in start_urls for keyword in [\\'black money\\',\\n                                                                    \\'money laundering\\', \\n                                                                    \\'money launder\\', \\n                                                                    \\'lauder the money\\', \\n                                                                    \\'money-mule\\', \\n                                                                    \\'money mule\\', \\n                                                                    \\'Hawala\\', \\n                                                   ...\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-97>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'news/news/spiders/news_spider.py'"]}]},{"cell_type":"code","metadata":{"id":"OHlIOEeVwaOY"},"source":["%cd /content/news/\r\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpQ_yShho4f3"},"source":["%%writefile websites.csv\r\n","https://economictimes.indiatimes.com/topic/,\r\n","https://timesofindia.indiatimes.com/topic/,\r\n","https://www.ndtv.com/search?searchtext="],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4MW8MxRwpoQ"},"source":["!scrapy crawl news"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVzMoqMYx0G7"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWivCbWrgkME"},"source":["%%writefile economictimes/economictimes/spiders/articles.py\r\n","import scrapy\r\n","import os\r\n","import csv\r\n","\r\n","from bs4 import BeautifulSoup\r\n","# from unidecode import unidecode\r\n","# import spacy\r\n","\r\n","# nlp_Name = spacy.load(\"en_core_web_trf\") # spacy.load(OUTPUT1)\r\n","\r\n","# csv_columns = ['text', 'name']\r\n","\r\n","# defining new variable passing two parameters\r\n","# writer = csv.DictWriter(open(os.path.abspath(os.path.join(os.getcwd(), 'test_results1.csv')), 'w'), fieldnames=csv_columns)\r\n","\r\n","# writeheader() method to the write to the file object\r\n","# writer.writeheader()\r\n","\r\n","class IndiaTestSpider(scrapy.Spider):\r\n","    name = \"articles\"\r\n","\r\n","    def start_requests(self):\r\n","        urls = [\r\n","            'https://economictimes.indiatimes.com/topic/hdfc-bank-news',\r\n","            # 'http://quotes.toscrape.com/page/2/',\r\n","        ]\r\n","        for url in urls:\r\n","            yield scrapy.Request(url=url, callback=self.parse)\r\n","\r\n","    def parse(self, response):\r\n","\r\n","      articles = {}\r\n","      # text data of entire webpage\r\n","      soup = BeautifulSoup(response.text, 'html.parser') # parse\r\n","      # extract body from it\r\n","      soup = soup.body # scrape.parse_soup\r\n","\r\n","      articles = soup.findAll(\"div\", {\"data-brcount\": \"24\"})\r\n","\r\n","      details = [paragraph for paragraph in articles.stripped_strings]\r\n","\r\n","      details = ' '.join(details)\r\n","\r\n","      articles['text'] = details\r\n","\r\n","      yield articles\r\n","\r\n","      # news_dict = {}\r\n","\r\n","      # links = []\r\n","\r\n","      # all_links = soup.findAll(\"a\", {'target': '_blank'})\r\n","\r\n","      # for link in all_links:\r\n","      #   headline = link.find('h2')\r\n","      #   if headline:\r\n","      #     news_dict['headline'] = headline.string\r\n","      #     news_dict['url'] = response.urljoin(link['href'])\r\n","      #     links.append(response.urljoin(link['href']))\r\n","\r\n","      #     yield news_dict\r\n","      \r\n","      # for _link in links:\r\n","      #   yield _link\r\n","\r\n","      # if len(soup.findAll('td'))==0:\r\n","      #   blocks = soup.findAll('div')\r\n","      # if len(soup.findAll('th'))==0: # this is the format\r\n","      #   blocks = soup.findAll('td') # of council of ministers webpage structure\r\n","      # else:\r\n","      #   blocks = soup.findAll('tr')\r\n","\r\n","      # if len(soup.findAll('td'))==0:\r\n","      #   blocks = soup.findAll('div')\r\n","      # elif len(soup.findAll('th'))==0: # this is the format\r\n","      #   blocks = soup.findAll('td') # of council of ministers webpage structure\r\n","      # else:\r\n","      #   blocks = soup.findAll('tr')\r\n","      \r\n","      # blocks = list(set(blocks))\r\n","      \r\n","      # for block in blocks:\r\n","        # save into a dictionary\r\n","        # profile = {'text': '', 'name': ''}\r\n","        # content = block.get_text(separator=' ', strip=True).replace('\\n', ' ').replace('\\t', ' ')\r\n","        # content = unidecode(content)\r\n","        # print(content)\r\n","        # profile['text'] += content + ' '\r\n","        \r\n","        # name=''\r\n","        # doc = nlp_Name(content)\r\n","        # profile['text'] = content\r\n","        # for count,ent in enumerate(doc.ents):\r\n","          # if ent.label_ == 'PERSON':\r\n","            # profile['name'] +=ent.text+' '\r\n","            # if count==0:\r\n","              # break\r\n","        \r\n","        # writer.writerow(profile)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOJqW1vpTgJx"},"source":["%cd economictimes/\r\n","!scrapy crawl news -O news.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7n1sCkwsWo9Q"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrTHxbKAXInR"},"source":[""],"execution_count":null,"outputs":[]}]}